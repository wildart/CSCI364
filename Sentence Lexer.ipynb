{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sentence Lexical Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Write a lexer to tokenize a sentence to words and quote skiping all \"white spaces\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "type STATE =\n",
    "     | START  = 0\n",
    "     | PLAIN  = 1    // Creating an identifier token\n",
    "     | QUOTED = 2    // Creating a string token\n",
    "     | EOF    = 3\n",
    "\n",
    "type TOKEN = EMPTY | WORD | QUOTE | KWORD\n",
    "\n",
    "// Simple function that classifies a character as being alphabetic or not.\n",
    "let Alpha = function\n",
    "    | X when X >= 'a' && X <= 'z' -> true\n",
    "    | X when X >= 'A' && X <= 'Z' -> true\n",
    "    | _ -> false\n",
    "\n",
    "// Simple function that classifies a character as being white space or not.\n",
    "let Space = function\n",
    "    |' ' -> true\n",
    "    |':' -> true\n",
    "    |'.' -> true\n",
    "    |',' -> true\n",
    "    | _ -> false\n",
    "\n",
    "// Simple function that classifies a character as being a quote or not\n",
    "let Quote = function\n",
    "    |'\"' -> true\n",
    "    | _ -> false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "// Some helper functions\n",
    "let append S C = S + C.ToString() // Append `char` to a `string`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "// Tokenizer\n",
    "let rec tokenize ((input:List<char>), state, lexeme, token) = \n",
    "    match (input, state) with\n",
    "    | (C::S, STATE.START)  when Space C -> tokenize (S, state, lexeme, TOKEN.EMPTY)\n",
    "    | (C::S, STATE.START)  when Alpha C -> tokenize (S, STATE.PLAIN, append \"\" C, TOKEN.WORD)\n",
    "    | (C::S, STATE.START)  when Quote C -> tokenize (S, STATE.QUOTED, \"\", TOKEN.QUOTE)\n",
    "    | (C::S, STATE.PLAIN)  when Space C -> (S, STATE.START, lexeme, TOKEN.WORD)\n",
    "    | (C::S, STATE.PLAIN)  when Alpha C -> tokenize (S, state, append lexeme C, TOKEN.WORD)\n",
    "    | (C::S, STATE.QUOTED) when Quote C -> (S, STATE.START, lexeme, TOKEN.QUOTE)\n",
    "    | (C::S, STATE.QUOTED) when Alpha C -> tokenize (S, state, append lexeme C, TOKEN.QUOTE)\n",
    "    | _                                 -> ([],STATE.EOF, \"\", TOKEN.EMPTY)\n",
    "\n",
    "// This function is passed an argument and expected to compute the 'next' token item in the sequence.\n",
    "let tokenizeNext value =\n",
    "    match value with\n",
    "    | [], _, _, _ -> None\n",
    "    | _ -> \n",
    "        let token = tokenize value\n",
    "        Some(token, token)\n",
    "        \n",
    "// Defines a sequence using the unfolder function and beginning with the initialized start token value.\n",
    "let tokens source =\n",
    "    let context = (Seq.toList source, STATE.START, \"\", TOKEN.EMPTY)\n",
    "    Seq.unfold (tokenizeNext) context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[  ,  , T, h, i, s,  , i, s,  , a,  , &quot;, t, e, s, t, &quot;,  , s ... (9 more) ]</div></td><td><span>START</span></td><td></td><td><div class=\"dni-plaintext\">EMPTY</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Create an initial quad containing the source text and an empty \n",
    "// lexeme and start state, the bool indicates if a lexme is a \n",
    "// quoted string.\n",
    "let source = \"\"\"  This is a \"test\" sentance .\"\"\"\n",
    "let context = (Seq.toList source, STATE.START, \"\", TOKEN.EMPTY)\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[ i, s,  , a,  , &quot;, t, e, s, t, &quot;,  , s, e, n, t, a, n, c, e ... (2 more) ]</div></td><td><span>START</span></td><td><div class=\"dni-plaintext\">This</div></td><td><div class=\"dni-plaintext\">WORD</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Invoke the tokenizer several times, the constructed lexeme (a string) \n",
    "// is contained within the returned triple and that triple is passed in \n",
    "// again to retrieve the next token and so on...\n",
    "\n",
    "let result1 = tokenize context\n",
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[ a,  , &quot;, t, e, s, t, &quot;,  , s, e, n, t, a, n, c, e,  , . ]</div></td><td><span>START</span></td><td><div class=\"dni-plaintext\">is</div></td><td><div class=\"dni-plaintext\">WORD</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let result2 = tokenize result1\n",
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[ &quot;, t, e, s, t, &quot;,  , s, e, n, t, a, n, c, e,  , . ]</div></td><td><span>START</span></td><td><div class=\"dni-plaintext\">a</div></td><td><div class=\"dni-plaintext\">WORD</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let result3 = tokenize result2\n",
    "result3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[  , s, e, n, t, a, n, c, e,  , . ]</div></td><td><span>START</span></td><td><div class=\"dni-plaintext\">test</div></td><td><div class=\"dni-plaintext\">QUOTE</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let result4 = tokenize result3\n",
    "result4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[ . ]</div></td><td><span>START</span></td><td><div class=\"dni-plaintext\">sentance</div></td><td><div class=\"dni-plaintext\">WORD</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let result5 = tokenize result4\n",
    "result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><thead><tr><th>Item1</th><th>Item2</th><th>Item3</th><th>Item4</th></tr></thead><tbody><tr><td><div class=\"dni-plaintext\">[  ]</div></td><td><span>EOF</span></td><td></td><td><div class=\"dni-plaintext\">EMPTY</div></td></tr></tbody></table>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize result5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: 'This', next state: START\n",
      "WORD: 'is', next state: START\n",
      "WORD: 'a', next state: START\n",
      "QUOTE: 'test', next state: START\n",
      "WORD: 'sentance', next state: START\n",
      "WORD: 'and', next state: START\n",
      "WORD: 'a', next state: START\n",
      "WORD: 'qoute', next state: START\n",
      "QUOTE: 'AAA', next state: START\n",
      "EMPTY: '', next state: EOF\n"
     ]
    }
   ],
   "source": [
    "let text_string = \"\"\"This is a \"test\" sentance, and a qoute: \"AAA\".\"\"\"\n",
    "for token in tokens text_string do\n",
    "    let _, state, lexeme, token = token\n",
    "    printfn \"%A: '%s', next state: %A\" token lexeme state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reserved words\n",
    "\n",
    "- Reserved words and identifiers can be recognized together\n",
    "    - rather than having a part of the diagram for each reserved word\n",
    "- Use a table lookup to determine whether a possible identifier is in fact a reserved word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: 'This', next state: START\n",
      "KWORD: 'is', next state: START\n",
      "WORD: 'a', next state: START\n",
      "QUOTE: 'test', next state: START\n",
      "WORD: 'sentance', next state: START\n",
      "WORD: 'and', next state: START\n",
      "WORD: 'a', next state: START\n",
      "WORD: 'qoute', next state: START\n",
      "KWORD: 'is', next state: START\n",
      "QUOTE: 'AAA', next state: START\n",
      "EMPTY: '', next state: EOF\n"
     ]
    }
   ],
   "source": [
    "let text_string = \"\"\"This is a \"test\" sentance, and a qoute is \"AAA\".\"\"\"\n",
    "for token in tokens text_string do\n",
    "    let _, state, lexeme, token = token\n",
    "    let newtkn = if lexeme = \"is\" then TOKEN.KWORD else token\n",
    "    printfn \"%A: '%s', next state: %A\" newtkn lexeme state"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".NET (F#)",
   "language": "F#",
   "name": ".net-fsharp"
  },
  "language": "fsharp",
  "language_info": {
   "file_extension": ".fs",
   "mimetype": "text/x-fsharp",
   "name": "C#",
   "pygments_lexer": "fsharp",
   "version": "4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
