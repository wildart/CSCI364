{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Sentence Lexical Analyzer"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "Write a lexer to tokenize a sentence to words and quote skiping all \"white spaces\"."
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "type STATE =\n     | START  = 0\n     | PLAIN  = 1    // Creating an identifier token\n     | QUOTED = 2    // Creating a string token\n     | EOF    = 3\n\ntype TOKEN = EMPTY | WORD | QUOTE | KWORD\n\n// Simple function that classifies a character as being alphabetic or not.\nlet Alpha = function\n    | X when X >= 'a' && X <= 'z' -> true\n    | X when X >= 'A' && X <= 'Z' -> true\n    | _ -> false\n\n// Simple function that classifies a character as being white space or not.\nlet Space = function\n    |' ' -> true\n    |':' -> true\n    |'.' -> true\n    |',' -> true\n    | _ -> false\n\n// Simple function that classifies a character as being a quote or not\nlet Quote = function\n    |'\"' -> true\n    | _ -> false",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Some helper functions\nlet append S C = S + C.ToString() // Append `char` to a `string`.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "code",
      "source": "// Tokenizer\nlet rec tokenize ((input:List<char>), state, lexeme, token) = \n    match (input, state) with\n    | (C::S, STATE.START)  when Space C -> tokenize (S, state, lexeme, TOKEN.EMPTY)\n    | (C::S, STATE.START)  when Alpha C -> tokenize (S, STATE.PLAIN, append \"\" C, TOKEN.WORD)\n    | (C::S, STATE.START)  when Quote C -> tokenize (S, STATE.QUOTED, \"\", TOKEN.QUOTE)\n    | (C::S, STATE.PLAIN)  when Space C -> (S, STATE.START, lexeme, TOKEN.WORD)\n    | (C::S, STATE.PLAIN)  when Alpha C -> tokenize (S, state, append lexeme C, TOKEN.WORD)\n    | (C::S, STATE.QUOTED) when Quote C -> (S, STATE.START, lexeme, TOKEN.QUOTE)\n    | (C::S, STATE.QUOTED) when Alpha C -> tokenize (S, state, append lexeme C, TOKEN.QUOTE)\n    | _                                 -> ([],STATE.EOF, \"\", TOKEN.EMPTY)\n\n// This function is passed an argument and expected to compute the 'next' token item in the sequence.\nlet tokenizeNext value =\n    match value with\n    | [], _, _, _ -> None\n    | _ -> \n        let token = tokenize value\n        Some(token, token)\n        \n// Defines a sequence using the unfolder function and beginning with the initialized start token value.\nlet tokens source =\n    let context = (Seq.toList source, STATE.START, \"\", TOKEN.EMPTY)\n    Seq.unfold (tokenizeNext) context",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "code",
      "source": "// Create an initial quad containing the source text and an empty \n// lexeme and start state, the bool indicates if a lexme is a \n// quoted string.\nlet source = \"\"\"  This is a \"test\" sentance.\"\"\"\nlet context = (Seq.toList source, STATE.START, \"\", TOKEN.EMPTY)\ncontext",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "([' '; ' '; 'T'; 'h'; 'i'; 's'; ' '; 'i'; 's'; ' '; 'a'; ' '; '\"'; 't'; 'e'; 's';\n  't'; '\"'; ' '; 's'; 'e'; 'n'; 't'; 'a'; 'n'; 'c'; 'e'; '.'], START, \"\", EMPTY)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "code",
      "source": "// Invoke the tokenizer several times, the constructed lexeme (a string) \n// is contained within the returned triple and that triple is passed in \n// again to retrieve the next token and so on...\n\nlet result1 = tokenize context\nresult1",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "(['i'; 's'; ' '; 'a'; ' '; '\"'; 't'; 'e'; 's'; 't'; '\"'; ' '; 's'; 'e'; 'n'; 't';\n  'a'; 'n'; 'c'; 'e'; '.'], START, \"This\", WORD)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "fragment"
        }
      },
      "cell_type": "code",
      "source": "let result2 = tokenize result1\nresult2",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "(['a'; ' '; '\"'; 't'; 'e'; 's'; 't'; '\"'; ' '; 's'; 'e'; 'n'; 't'; 'a'; 'n'; 'c';\n  'e'; '.'], START, \"is\", WORD)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "code",
      "source": "let result3 = tokenize result2\nresult3",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 18,
          "data": {
            "text/plain": "(['\"'; 't'; 'e'; 's'; 't'; '\"'; ' '; 's'; 'e'; 'n'; 't'; 'a'; 'n'; 'c'; 'e'; '.'],\n START, \"a\", WORD)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "code",
      "source": "let result4 = tokenize result3\nresult4",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 19,
          "data": {
            "text/plain": "([' '; 's'; 'e'; 'n'; 't'; 'a'; 'n'; 'c'; 'e'; '.'], START, \"test\", QUOTE)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "skip"
        }
      },
      "cell_type": "code",
      "source": "let result5 = tokenize result4\nresult5",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 20,
          "data": {
            "text/plain": "([], START, \"sentance\", WORD)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "tokenize result5",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 21,
          "data": {
            "text/plain": "([], EOF, \"\", EMPTY)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "let text_string = \"\"\"This is a \"test\" sentance, and a qoute: \"AAA\".\"\"\"\nfor token in tokens text_string do\n    let _, state, lexeme, token = token\n    printfn \"%A: '%s', next state: %A\" token lexeme state",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "WORD: 'This', next state: START\nWORD: 'is', next state: START\nWORD: 'a', next state: START\nQUOTE: 'test', next state: START\nWORD: 'sentance', next state: START\nWORD: 'and', next state: START\nWORD: 'a', next state: START\nWORD: 'qoute', next state: START\nQUOTE: 'AAA', next state: START\nEMPTY: '', next state: EOF\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Reserved words\n\n- Reserved words and identifiers can be recognized together\n    - rather than having a part of the diagram for each reserved word\n- Use a table lookup to determine whether a possible identifier is in fact a reserved word"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "let text_string = \"\"\"This is a \"test\" sentance, and a qoute: \"AAA\".\"\"\"\nfor token in tokens text_string do\n    let _, state, lexeme, token = token\n    let newtkn = if lexeme = \"is\" then TOKEN.KWORD else token\n    printfn \"%A: '%s', next state: %A\" newtkn lexeme state",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "WORD: 'This', next state: START\nKWORD: 'is', next state: START\nWORD: 'a', next state: START\nQUOTE: 'test', next state: START\nWORD: 'sentance', next state: START\nWORD: 'and', next state: START\nWORD: 'a', next state: START\nWORD: 'qoute', next state: START\nQUOTE: 'AAA', next state: START\nEMPTY: '', next state: EOF\n"
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ifsharp",
      "display_name": "F#",
      "language": "fsharp"
    },
    "language_info": {
      "mimetype": "text/x-fsharp",
      "nbconvert_exporter": "",
      "name": "fsharp",
      "pygments_lexer": "",
      "version": "4.3.1.0",
      "file_extension": ".fs",
      "codemirror_mode": ""
    },
    "language": "fsharp"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}