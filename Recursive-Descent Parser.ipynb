{
  "cells": [
    {
      "metadata": {
        "collapsed": true,
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "# Recursive-Descent Parser"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Problem\n\nConsider the following EBNF description of simple arithmetic expressions:\n```\n// <expr> → <term> | <term> + <term> | <term> - <term>\n<expr> → <term> {( + | - ) <term>}     \n// <term> → <factor> | <factor> * <factor> | <factor> / <factor> \n<term> → <factor> {( * | / ) <factor>} \n<factor> → id | int_lit | ( <expr> )\n```\n\nSuppose a lexical analyzer recognizes:\n- only arithmetic expressions which include operations: `+`, `-`, `*`, `/` \n- integer literals\n- parentheses\n- variable names which consist of strings of uppercase or lowercase letters\n    - Optional: might contain digits but must begin with a letter.\n- white spaces are ignored   \n    \nWrite a **lexical** and **syntactic** analyzers that process arithmetical expressions.\n- Example: **(sum + 47) / total**"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Lexical Analyzer\n\n- Define `STATE` type that describes states of lexer finite automaton\n- Define `TOKEN` type that lists of token in our language\n- Provide functions for identifying character types: alphabet, number, special characters or white space"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Lexer finite-state automaton states\ntype STATE = START | IDENT | NUMBR | EOF\n     \n// Tokens\ntype TOKEN = INT_LIT | IDENT | ASSIGN_OP | ADD_OP | SUB_OP | MULT_OP | DIV_OP | LEFT_PAREN | RIGHT_PAREN | UNKNOWN | EOF\n\n// Simple function that classifies a character as being alphabetic or not.\nlet Alpha = function\n    | X when X >= 'a' && X <= 'z' -> true\n    | X when X >= 'A' && X <= 'Z' -> true\n    | _ -> false\n\n// Simple function that classifies a character as being number or not.\nlet Number = function\n    | X when X >= '0' && X <= '9' -> true\n    | _ -> false\n    \n// Simple function that classifies a character as being white space or not.\nlet Space = function\n    |' ' | '\\t' -> true\n    | _ -> false",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "Some active patterns for detecting binary operations and parentheses"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "fragment"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Simple function that classifies a character as being parentheses or not.\nlet (|Parent|_|) input =\n    match input with\n    |'(' -> Some(LEFT_PAREN)\n    |')' -> Some(RIGHT_PAREN)\n    | _  -> None\n\n// Active pattern for detecting binary operations\nlet (|BinOp|_|) input =\n    match input with\n    |'+' -> Some(ADD_OP)\n    |'-' -> Some(SUB_OP)\n    |'/' -> Some(DIV_OP)\n    |'*' -> Some(MULT_OP)\n    |'=' -> Some(ASSIGN_OP)\n    | _  -> None\n\n// Append `char` to a `string`.\nlet append S C = S + C.ToString() ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Tokenizer: implementation of FSA\nlet rec tokenize ((source:seq<char>), state, lexeme, token) =\n    if Seq.isEmpty source && state = STATE.EOF then\n        (Seq.empty<char>, STATE.EOF, \"\", EOF)       // Last token in tokenizer, repeat it infinitly\n    else if Seq.isEmpty source then\n        (Seq.empty<char>, STATE.EOF, lexeme, token) // Indicate end of input\n    else\n        let C = Seq.head source\n        let S = Seq.tail source\n        match (C, state) with\n        | (C       , STATE.START) when Space C   -> tokenize (S, state, \"\", token)\n        | (BinOp Op, STATE.START)                -> (S, state, append \"\" C, Op)\n        | (Parent P, STATE.START)                -> (S, state, append \"\" C, P)\n        | (C       , STATE.START) when Alpha C   -> tokenize (S, STATE.IDENT, append \"\" C, TOKEN.IDENT)\n        | (C       , STATE.START) when Number C  -> tokenize (S, STATE.NUMBR, append \"\" C, TOKEN.INT_LIT)\n        | (C       , STATE.IDENT) when Alpha C   -> tokenize (S, state, append lexeme C, TOKEN.IDENT)\n        | (C       , STATE.IDENT) when Space C   -> (S, STATE.START, lexeme, TOKEN.IDENT)\n        | (BinOp Op, STATE.IDENT)                -> (source, STATE.START, lexeme, TOKEN.IDENT)\n        | (Parent P, STATE.IDENT)                -> (source, STATE.START, lexeme, TOKEN.IDENT)\n        | (C       , STATE.NUMBR) when Space C   -> (S, STATE.START, lexeme, TOKEN.INT_LIT)\n        | (C       , STATE.NUMBR) when Number C  -> tokenize (S, state, append lexeme C, TOKEN.INT_LIT)\n        | (BinOp Op, STATE.NUMBR)                -> (source, STATE.START, lexeme, TOKEN.INT_LIT)\n        | (Parent P, STATE.NUMBR)                -> (source, STATE.START, lexeme, TOKEN.INT_LIT)\n        | _                                      -> (Seq.empty<char>, STATE.EOF, append lexeme C, TOKEN.UNKNOWN)      ",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Form a sequence from a tokenizer calls\nlet token_sequence source =\n    let state = tokenize (source, STATE.START, \"\", TOKEN.UNKNOWN)\n    let unfolder current_state =\n        let _, st, lex, tkn = current_state\n        if tkn = TOKEN.EOF then\n            None\n        else\n            let next_state = tokenize current_state\n            Some(current_state, next_state)\n    Seq.unfold unfolder state",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Test some inputs\nlet correct_input = \"(sum + 47)/ total\"\n\nfor state in token_sequence correct_input do\n    let _, st, lex, tkn = state\n    printfn \"State: %A,\\tLexem: %s,\\tToken: %A\" st lex tkn",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "State: START,\tLexem: (,\tToken: LEFT_PAREN\nState: START,\tLexem: sum,\tToken: IDENT\nState: START,\tLexem: +,\tToken: ADD_OP\nState: START,\tLexem: 47,\tToken: INT_LIT\nState: START,\tLexem: ),\tToken: RIGHT_PAREN\nState: START,\tLexem: /,\tToken: DIV_OP\nState: EOF,\tLexem: total,\tToken: IDENT\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// This input contains error: `47?` is not a proper integer literal\nlet error_input = \"(sum + 47?)/ total\"\n\nfor state in token_sequence error_input do\n    let _, st, lex, tkn = state\n    printfn \"State: %A,\\tLexem: %s,\\tToken: %A\" st lex tkn",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "State: START,\tLexem: (,\tToken: LEFT_PAREN\nState: START,\tLexem: sum,\tToken: IDENT\nState: START,\tLexem: +,\tToken: ADD_OP\nState: EOF,\tLexem: 47?,\tToken: UNKNOWN\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "## Recursive-Descent Parsing\n\n- There is a subprogram for each nonterminal in the grammar, which can parse sentences that can be generated by that nonterminal\n- EBNF is ideally suited for being the basis for a recursive-descent parser, because EBNF  minimizes the number of nonterminals\n\nThe coding process when there is only one RHS:\n- For each terminal symbol in the RHS, compare it with the next input token; if they match, continue, else there is an error\n- For each nonterminal symbol in the RHS, call its associated parsing subprogram\n\nA nonterminal that has more than one RHS requires an initial process to determine which RHS it is to parse\n- The correct RHS is chosen on the basis of the next token of input (the lookahead)\n- The next token is compared with the first token that can be generated by each RHS until a match is found\n- If no match is found, it is a syntax error"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "let token (src, st, lex, tkn) = tkn // get a token from a context\n\nlet tokenize_print ctx =            // call tokenizer, print and return acquired token\n    let newctx = tokenize ctx\n    let _, st, lex, tkn = newctx\n    printfn \"%A: %s\" tkn lex\n    newctx  ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        }
      },
      "cell_type": "markdown",
      "source": "Implementation of a recursive-descent parser based of following grammar:\n```\n<expr> → <term> {( + | - ) <term>}\n<term> → <factor> {( * | / ) <factor>}\n<factor> → id | int_lit | ( <expr> )\n```\n\n- Implement `expr`, `term` & `factor` functions"
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "subslide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "// Parses strings in the language generated by the rule:\n//     <factor> -> id | int_constant | ( <expr> )\nlet rec factor context = \n    printfn \"Enter <factor>\"\n\n    // Determine which RHS\n    let next_context = \n        let tkn = token context\n        if tkn = TOKEN.IDENT || tkn = INT_LIT then\n            // Get the next token\n            tokenize_print context\n        else\n            // If the RHS is (<expr>), pass over the left parenthesis,\n            // call expr, and check for the right parenthesis\n            if tkn = TOKEN.LEFT_PAREN then                \n                let expr_context = context |> tokenize_print  |> expr \n                if (token expr_context) = TOKEN.RIGHT_PAREN then                    \n                    tokenize_print expr_context\n                else\n                    failwith \"No right parenthesis\"\n            else\n                failwith \"Not an id, an integer literal, or a left parenthesis\"                                    \n\n    printfn \"Exit <factor>\"\n    next_context\n\n// As long as the next token is * or /, get the next token and parse the next factor\nand next_factor context = \n    match (token context) with\n    | TOKEN.MULT_OP | TOKEN.DIV_OP -> context |> tokenize_print |> factor |> next_factor\n    | _ -> context\n\n// Parses strings in the language generated by the rule:\n//     <term> -> <factor> {(* | /) <factor>)\nand term context =    \n    printfn \"Enter <term>\"\n    let next_context = context |> factor |> next_factor\n    printfn \"Exit <term>\"\n    next_context        \n\n// As long as the next token is + or -, get the next token and parse the next term\nand next_term context = \n    match (token context) with\n    | TOKEN.ADD_OP | TOKEN.SUB_OP ->  context |> tokenize_print |> term |> next_term\n    | _ -> context\n\n// Parses strings in the language generated by the rule:\n//     <expr> -> <term> {(+ | -) <term>}\nand expr context =\n    printfn \"Enter <expr>\"\n    let next_context = context |> term  |> next_term         \n    printfn \"Exit <expr>\"\n    next_context\n\n// Parser function\nlet parser source =\n    printfn \"Start parsing: %s\" source    \n    tokenize_print (source, STATE.START, \"\", TOKEN.UNKNOWN) |> expr |> ignore",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "skip"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "tokenize_print (correct_input, STATE.START, \"\", TOKEN.UNKNOWN)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "(seq ['s'; 'u'; 'm'; ' '; ...], START, \"(\", LEFT_PAREN)"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "LEFT_PAREN: (\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "correct_input",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "\"(sum + 47)/ total\""
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "parser correct_input",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Start parsing: (sum + 47)/ total\nLEFT_PAREN: (\nEnter <expr>\nEnter <term>\nEnter <factor>\nIDENT: sum\nEnter <expr>\nEnter <term>\nEnter <factor>\nADD_OP: +\nExit <factor>\nExit <term>\nINT_LIT: 47\nEnter <term>\nEnter <factor>\nRIGHT_PAREN: )\nExit <factor>\nExit <term>\nExit <expr>\nDIV_OP: /\nExit <factor>\nIDENT: total\nEnter <factor>\nEOF: \nExit <factor>\nExit <term>\nExit <expr>\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "slideshow": {
          "slide_type": "slide"
        },
        "trusted": true
      },
      "cell_type": "code",
      "source": "try\n    parser error_input\nwith\n | Failure(msg) -> printfn \"Error: %s\" msg",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Start parsing: (sum + 47?)/ total\nLEFT_PAREN: (\nEnter <expr>\nEnter <term>\nEnter <factor>\nIDENT: sum\nEnter <expr>\nEnter <term>\nEnter <factor>\nADD_OP: +\nExit <factor>\nExit <term>\nUNKNOWN: 47?\nEnter <term>\nEnter <factor>\nError: Not an id, an integer literal, or a left parenthesis\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "try\n    parser \"a +- b\"\nwith\n | Failure(msg) -> printfn \"Error: %s\" msg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "ifsharp",
      "display_name": "F#",
      "language": "fsharp"
    },
    "language_info": {
      "mimetype": "text/x-fsharp",
      "nbconvert_exporter": "",
      "name": "fsharp",
      "pygments_lexer": "",
      "version": "4.3.1.0",
      "file_extension": ".fs",
      "codemirror_mode": ""
    },
    "language": "fsharp",
    "celltoolbar": "Slideshow"
  },
  "nbformat": 4,
  "nbformat_minor": 2
}